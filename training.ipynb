{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implements a generic training loop.\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.autonotebook import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "def train(model, train_dataloader, epochs, lr, steps_til_summary, epochs_til_checkpoint, model_dir, loss_fn,\n",
        "          summary_fn=None, val_dataloader=None, double_precision=False, clip_grad=False, use_lbfgs=False, loss_schedules=None,\n",
        "          validation_fn=None, start_epoch=0):\n",
        "\n",
        "    optim = torch.optim.Adam(lr=lr, params=model.parameters())\n",
        "\n",
        "    # copy settings from Raissi et al. (2019) and here \n",
        "    # https://github.com/maziarraissi/PINNs\n",
        "    if use_lbfgs:\n",
        "        optim = torch.optim.LBFGS(lr=lr, params=model.parameters(), max_iter=50000, max_eval=50000,\n",
        "                                  history_size=50, line_search_fn='strong_wolfe')\n",
        "\n",
        "    # Load the checkpoint if required\n",
        "    if start_epoch > 0:\n",
        "        # Load the model and start training from that point onwards\n",
        "        model_path = os.path.join(model_dir, 'checkpoints', 'model_epoch_%04d.pth' % start_epoch)\n",
        "        checkpoint = torch.load(model_path)\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "        model.train()\n",
        "        optim.load_state_dict(checkpoint['optimizer'])\n",
        "        optim.param_groups[0]['lr'] = lr\n",
        "        assert(start_epoch == checkpoint['epoch'])\n",
        "    else:\n",
        "        # Start training from scratch\n",
        "        if os.path.exists(model_dir):\n",
        "            val = input(\"The model directory %s exists. Overwrite? (y/n)\"%model_dir)\n",
        "            if val == 'y':\n",
        "                shutil.rmtree(model_dir)\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    summaries_dir = os.path.join(model_dir, 'summaries')\n",
        "    utils.cond_mkdir(summaries_dir)\n",
        "\n",
        "    checkpoints_dir = os.path.join(model_dir, 'checkpoints')\n",
        "    utils.cond_mkdir(checkpoints_dir)\n",
        "\n",
        "    writer = SummaryWriter(summaries_dir)\n",
        "\n",
        "    total_steps = 0\n",
        "    with tqdm(total=len(train_dataloader) * epochs) as pbar:\n",
        "        train_losses = []\n",
        "        for epoch in range(start_epoch, epochs):\n",
        "            if not epoch % epochs_til_checkpoint and epoch:\n",
        "                # Saving the optimizer state is important to produce consistent results\n",
        "                checkpoint = { \n",
        "                    'epoch': epoch,\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optim.state_dict()}\n",
        "                torch.save(checkpoint,\n",
        "                           os.path.join(checkpoints_dir, 'model_epoch_%04d.pth' % epoch))\n",
        "                # torch.save(model.state_dict(),\n",
        "                #            os.path.join(checkpoints_dir, 'model_epoch_%04d.pth' % epoch))\n",
        "                np.savetxt(os.path.join(checkpoints_dir, 'train_losses_epoch_%04d.txt' % epoch),\n",
        "                           np.array(train_losses))\n",
        "                if validation_fn is not None:\n",
        "                    validation_fn(model, checkpoints_dir, epoch)\n",
        "\n",
        "            for step, (model_input, gt) in enumerate(train_dataloader):\n",
        "                start_time = time.time()\n",
        "            \n",
        "                model_input = {key: value.cuda() for key, value in model_input.items()}\n",
        "                gt = {key: value.cuda() for key, value in gt.items()}\n",
        "\n",
        "                if double_precision:\n",
        "                    model_input = {key: value.double() for key, value in model_input.items()}\n",
        "                    gt = {key: value.double() for key, value in gt.items()}\n",
        "\n",
        "                if use_lbfgs:\n",
        "                    def closure():\n",
        "                        optim.zero_grad()\n",
        "                        model_output = model(model_input)\n",
        "                        losses = loss_fn(model_output, gt)\n",
        "                        train_loss = 0.\n",
        "                        for loss_name, loss in losses.items():\n",
        "                            train_loss += loss.mean() \n",
        "                        train_loss.backward()\n",
        "                        return train_loss\n",
        "                    optim.step(closure)\n",
        "\n",
        "                model_output = model(model_input)\n",
        "                losses = loss_fn(model_output, gt)\n",
        "\n",
        "                # import ipdb; ipdb.set_trace()\n",
        "\n",
        "                train_loss = 0.\n",
        "                for loss_name, loss in losses.items():\n",
        "                    single_loss = loss.mean()\n",
        "\n",
        "                    if loss_schedules is not None and loss_name in loss_schedules:\n",
        "                        writer.add_scalar(loss_name + \"_weight\", loss_schedules[loss_name](total_steps), total_steps)\n",
        "                        single_loss *= loss_schedules[loss_name](total_steps)\n",
        "\n",
        "                    writer.add_scalar(loss_name, single_loss, total_steps)\n",
        "                    train_loss += single_loss\n",
        "\n",
        "                train_losses.append(train_loss.item())\n",
        "                writer.add_scalar(\"total_train_loss\", train_loss, total_steps)\n",
        "\n",
        "                if not total_steps % steps_til_summary:\n",
        "                    torch.save(model.state_dict(),\n",
        "                               os.path.join(checkpoints_dir, 'model_current.pth'))\n",
        "                    # summary_fn(model, model_input, gt, model_output, writer, total_steps)\n",
        "\n",
        "                if not use_lbfgs:\n",
        "                    optim.zero_grad()\n",
        "                    train_loss.backward()\n",
        "\n",
        "                    if clip_grad:\n",
        "                        if isinstance(clip_grad, bool):\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "                        else:\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad)\n",
        "\n",
        "                    optim.step()\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                if not total_steps % steps_til_summary:\n",
        "                    tqdm.write(\"Epoch %d, Total loss %0.6f, iteration time %0.6f\" % (epoch, train_loss, time.time() - start_time))\n",
        "\n",
        "                    if val_dataloader is not None:\n",
        "                        print(\"Running validation set...\")\n",
        "                        model.eval()\n",
        "                        with torch.no_grad():\n",
        "                            val_losses = []\n",
        "                            for (model_input, gt) in val_dataloader:\n",
        "                                model_output = model(model_input)\n",
        "                                val_loss = loss_fn(model_output, gt)\n",
        "                                val_losses.append(val_loss)\n",
        "\n",
        "                            writer.add_scalar(\"val_loss\", np.mean(val_losses), total_steps)\n",
        "                        model.train()\n",
        "\n",
        "                total_steps += 1\n",
        "\n",
        "        torch.save(model.state_dict(),\n",
        "                   os.path.join(checkpoints_dir, 'model_final.pth'))\n",
        "        np.savetxt(os.path.join(checkpoints_dir, 'train_losses_final.txt'),\n",
        "                   np.array(train_losses))\n",
        "\n",
        "\n",
        "class LinearDecaySchedule():\n",
        "    def __init__(self, start_val, final_val, num_steps):\n",
        "        self.start_val = start_val\n",
        "        self.final_val = final_val\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "    def __call__(self, iter):\n",
        "        return self.start_val + (self.final_val - self.start_val) * min(iter / self.num_steps, 1.)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}