{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "\n",
        "class BatchLinear(nn.Linear):\n",
        "    '''A linear layer'''\n",
        "    __doc__ = nn.Linear.__doc__\n",
        "\n",
        "    def forward(self, input, params=None):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "\n",
        "        bias = params.get('bias', None)\n",
        "        weight = params['weight']\n",
        "\n",
        "        output = input.matmul(weight.permute(*[i for i in range(len(weight.shape) - 2)], -1, -2))\n",
        "        output += bias.unsqueeze(-2)\n",
        "        return output\n",
        "\n",
        "\n",
        "class Sine(nn.Module):\n",
        "    def __init(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30\n",
        "        return torch.sin(30 * input)\n",
        "\n",
        "\n",
        "class FCBlock(nn.Module):\n",
        "    '''A fully connected neural network.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, in_features, out_features, num_hidden_layers, hidden_features,\n",
        "                 outermost_linear=False, nonlinearity='relu', weight_init=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.first_layer_init = None\n",
        "\n",
        "        # Dictionary that maps nonlinearity name to the respective function, initialization, and, if applicable,\n",
        "        # special first-layer initialization scheme\n",
        "        nls_and_inits = {'sine':(Sine(), sine_init, first_layer_sine_init),\n",
        "                         'relu':(nn.ReLU(inplace=True), init_weights_normal, None),\n",
        "                         'sigmoid':(nn.Sigmoid(), init_weights_xavier, None),\n",
        "                         'tanh':(nn.Tanh(), init_weights_xavier, None),\n",
        "                         'selu':(nn.SELU(inplace=True), init_weights_selu, None),\n",
        "                         'softplus':(nn.Softplus(), init_weights_normal, None),\n",
        "                         'elu':(nn.ELU(inplace=True), init_weights_elu, None)}\n",
        "\n",
        "        nl, nl_weight_init, first_layer_init = nls_and_inits[nonlinearity]\n",
        "\n",
        "        if weight_init is not None:  # Overwrite weight init if passed\n",
        "            self.weight_init = weight_init\n",
        "        else:\n",
        "            self.weight_init = nl_weight_init\n",
        "\n",
        "        self.net = []\n",
        "        self.net.append(nn.Sequential(\n",
        "            BatchLinear(in_features, hidden_features), nl\n",
        "        ))\n",
        "\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.net.append(nn.Sequential(\n",
        "                BatchLinear(hidden_features, hidden_features), nl\n",
        "            ))\n",
        "\n",
        "        if outermost_linear:\n",
        "            self.net.append(nn.Sequential(BatchLinear(hidden_features, out_features)))\n",
        "        else:\n",
        "            self.net.append(nn.Sequential(\n",
        "                BatchLinear(hidden_features, out_features), nl\n",
        "            ))\n",
        "\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "        if self.weight_init is not None:\n",
        "            self.net.apply(self.weight_init)\n",
        "\n",
        "        if first_layer_init is not None: # Apply special initialization to first layer, if applicable.\n",
        "            self.net[0].apply(first_layer_init)\n",
        "\n",
        "    def forward(self, coords, params=None, **kwargs):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "\n",
        "        output = self.net(coords)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SingleBVPNet(nn.Module):\n",
        "    '''A canonical representation network for a BVP.'''\n",
        "\n",
        "    def __init__(self, out_features=1, type='sine', in_features=2,\n",
        "                 mode='mlp', hidden_features=256, num_hidden_layers=3, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        self.net = FCBlock(in_features=in_features, out_features=out_features, num_hidden_layers=num_hidden_layers,\n",
        "                           hidden_features=hidden_features, outermost_linear=True, nonlinearity=type)\n",
        "        print(self)\n",
        "\n",
        "    def forward(self, model_input, params=None):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "\n",
        "        # Enables us to compute gradients w.r.t. coordinates\n",
        "        coords_org = model_input['coords'].clone().detach().requires_grad_(True)\n",
        "        coords = coords_org\n",
        "\n",
        "        output = self.net(coords)\n",
        "        return {'model_in': coords_org, 'model_out': output}\n",
        "\n",
        "\n",
        "########################\n",
        "# Initialization methods\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # For PINNet, Raissi et al. 2019\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    # grab from upstream pytorch branch and paste here for now\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Values are generated by using a truncated uniform distribution and\n",
        "        # then using the inverse CDF for the normal distribution.\n",
        "        # Get upper and lower cdf values\n",
        "        l = norm_cdf((a - mean) / std)\n",
        "        u = norm_cdf((b - mean) / std)\n",
        "\n",
        "        # Uniformly fill tensor with values from [l, u], then translate to\n",
        "        # [2l-1, 2u-1].\n",
        "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
        "\n",
        "        # Use inverse cdf transform for normal distribution to get truncated\n",
        "        # standard normal\n",
        "        tensor.erfinv_()\n",
        "\n",
        "        # Transform to proper mean, std\n",
        "        tensor.mul_(std * math.sqrt(2.))\n",
        "        tensor.add_(mean)\n",
        "\n",
        "        # Clamp to ensure it's in the proper range\n",
        "        tensor.clamp_(min=a, max=b)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def init_weights_trunc_normal(m):\n",
        "    # For PINNet, Raissi et al. 2019\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    if type(m) == BatchLinear or type(m) == nn.Linear:\n",
        "        if hasattr(m, 'weight'):\n",
        "            fan_in = m.weight.size(1)\n",
        "            fan_out = m.weight.size(0)\n",
        "            std = math.sqrt(2.0 / float(fan_in + fan_out))\n",
        "            mean = 0.\n",
        "            # initialize with the same behavior as tf.truncated_normal\n",
        "            # \"The generated values follow a normal distribution with specified mean and\n",
        "            # standard deviation, except that values whose magnitude is more than 2\n",
        "            # standard deviations from the mean are dropped and re-picked.\"\n",
        "            _no_grad_trunc_normal_(m.weight, mean, std, -2 * std, 2 * std)\n",
        "\n",
        "\n",
        "def init_weights_normal(m):\n",
        "    if type(m) == BatchLinear or type(m) == nn.Linear:\n",
        "        if hasattr(m, 'weight'):\n",
        "            nn.init.kaiming_normal_(m.weight, a=0.0, nonlinearity='relu', mode='fan_in')\n",
        "\n",
        "\n",
        "def init_weights_selu(m):\n",
        "    if type(m) == BatchLinear or type(m) == nn.Linear:\n",
        "        if hasattr(m, 'weight'):\n",
        "            num_input = m.weight.size(-1)\n",
        "            nn.init.normal_(m.weight, std=1 / math.sqrt(num_input))\n",
        "\n",
        "\n",
        "def init_weights_elu(m):\n",
        "    if type(m) == BatchLinear or type(m) == nn.Linear:\n",
        "        if hasattr(m, 'weight'):\n",
        "            num_input = m.weight.size(-1)\n",
        "            nn.init.normal_(m.weight, std=math.sqrt(1.5505188080679277) / math.sqrt(num_input))\n",
        "\n",
        "\n",
        "def init_weights_xavier(m):\n",
        "    if type(m) == BatchLinear or type(m) == nn.Linear:\n",
        "        if hasattr(m, 'weight'):\n",
        "            nn.init.xavier_normal_(m.weight)\n",
        "\n",
        "\n",
        "def sine_init(m):\n",
        "    with torch.no_grad():\n",
        "        if hasattr(m, 'weight'):\n",
        "            num_input = m.weight.size(-1)\n",
        "            # See supplement Sec. 1.5 for discussion of factor 30\n",
        "            m.weight.uniform_(-np.sqrt(6 / num_input) / 30, np.sqrt(6 / num_input) / 30)\n",
        "\n",
        "\n",
        "def first_layer_sine_init(m):\n",
        "    with torch.no_grad():\n",
        "        if hasattr(m, 'weight'):\n",
        "            num_input = m.weight.size(-1)\n",
        "            # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of factor 30\n",
        "            m.weight.uniform_(-1 / num_input, 1 / num_input)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}